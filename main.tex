\documentclass{article}
% --- Packages ------------------------------------------- ----- Standard Packages --------------------------------
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm} %let us define theorems
\usepackage{amssymb} %Useful Symbols
\usepackage{mathtools} %better version of amsmath
\usepackage{microtype} %make things look nicer
\usepackage{float} %Let figures appear the right place on page
\usepackage{scalerel}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
% https://tex.stackexchange.com/questions/254044/caption-and-label-on-minted-code
\usepackage[newfloat]{minted}
\usepackage{caption}
\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Code Block}
% --- Style for haskell code
\usepackage{listings}
\lstset{
  frame=none,
  xleftmargin=2pt,
  stepnumber=1,
  numbers=left,
  numbersep=5pt,
  numberstyle=\ttfamily\tiny\color[gray]{0.3},
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  escapeinside={*'}{'*},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it,
  stringstyle=\mdseries\rmfamily,
  showspaces=false,
  keywordstyle=\bfseries\rmfamily,
  columns=flexible,
  basicstyle=\small\sffamily,
  showstringspaces=false,
  morecomment=[l]\%,
}
% ---- bibliography ----
\usepackage[backend = biber, style = alphabetic, urldate = long]{biblatex}
\bibliography{References.bib}
% ----- custom commands ----
\newcommand{\ors}{\overline{\text{rs}}}
\newcommand{\rs}{\text{rs}}
\newcommand{\oas}{\overline{\text{as}}}
\newcommand{\xo}{\overline{\text{x}}}
\newcommand{\as}{\text{as}}
\title{DPP project}
\author{Peter Adema \\
Sophus Valentin Willumsgaard}
\date{January 2025}

\begin{document}

\maketitle
\tableofcontents
\section{Introduction}
The purpose of this project is to implement the algorithm given in \cite{PPAD}, for reverse
differentiation of the scan operator in Futhark.
In this report,
we describe the procedure for reverse differentiation of a
scan operation given in \cite{PPAD},
and how it differs from the current implementation as described in
\cite{Futhark}.

We then show how we have implemented the procedure in the Futhark compiler,
and lastly give benchmarks to compare with the current implementation.
\section{Description of Algorithm}
We here give a description of the general-case procedures for automatic differentiation of
scan described in \cite{PPAD} and \cite{Futhark}.

We will use \(\odot\) to denote a general associative operator with neutral
element \(e_{\odot}\), defined either on \(\mathbb{R}\) or \(\mathbb{R}^{n}\).
Given an array of variables in the program \(\text{x}\),
we let \(\xo\) denote the array of adjoint values in AD.
\subsection{Non Parallel Procedure}
Our task is assuming we have a program containing the following definition
\begin{minted}[escapeinside=||]{futhark}
let rs = scan |$\odot$| |$e_{\odot}$| as
\end{minted}
to use the values of \(\ors, \rs, \as\) to calculate \(\oas\).
From the definition of scan, we have
%\begin{minted}[escapeinside=||]{futhark}
%	|$\rs [0] = \as[0]$|
%	|$\rs [i+1] = \rs[i] \odot \as[i+1]$|
%\end{minted} 
\begin{align*}
	\rs [0] = \as[0] \\
	\rs [i+1] = \rs[i] \odot \as[i+1]
\end{align*}
We see that the value of \(\rs[i]\) depend on \(\rs[j]\) for \(i>j\).
For this reason we introduce another array of adjoint vectors \(\xo\),
which is the same as \(\ors\) but with the contribution of the lower
values to the higher values recorded.
From the above formulas we then get

\begin{align*}
	\xo [n-1] = \rs [n-1]      \\
	\xo [i-1] = \frac{\partial (\rs [i-1] \odot \as[i])}{\partial \rs[i-1]}
	\cdot \xo[i]   + \ors[i-1] \\
	\oas [0] = \xo[0]          \\
	\oas [i] = \frac{\partial (\rs [i-1] \odot \as[i])}{\partial \as[i]}
	\cdot
	\xo[i]
\end{align*}
From these formulas, we can calculate \(\oas\) in two loops.
The first loop inductively calculates \(\xo\),
and the second loop calculate \(\oas\),
however this would not preserve parallelism. We will now describe how to
calculate these in a parallel way.
\subsection{Parallelization}
First of we note that,
given the calculation of \(\xo\),

\newpage
\section{Implementation}
The implementation of the algorithm described in \cite{PPAD} was driven by the \href{https://github.com/diku-dk/ifl23-revad-red-scan/blob/main/scan/scan-adj-comp.fut}{reference implementation} in Futhark code from \cite{Futhark}, with the structure of the compiler code globally following the Futhark reference code. There is one notable difference between the Futhark implementation and the compiler code, namely in the treatment of neutral elements for \lstinline{op_lifted}. In \cite{PPAD} it was noted that for the lifted operator as defined there, \lstinline{(e, e, 0)} would serve as a right-neutral element, allowing for a reverse-scan over the constructed array \lstinline{m} (with \lstinline{e} being the original neutral element). This approach leads to the simple definition of \lstinline{op_lift} from the Futhark reference code in Code Block \ref{code:fut-oplift}:
\begin{code}
	\begin{minted}[linenos]{futhark}
let z_term = op_bar_1 op (x1, a1, y2_h)
let z = plus z_term y1_h
in  (x1, op a1 a2, z)
\end{minted}
	\caption{Reference Futhark code for \lstinline{op_lift} (operator for scan calculating adjoints for scan intermediates rs/xs). \lstinline{op_bar_1} refers to differentiating \lstinline{op} with respect to its first argument.\\}
	\label{code:fut-oplift}
\end{code}
And, while we can similarly define the core operator in the compiler code (Code Block \ref{code:hsk-oplift}):
\begin{code}
	\begin{minted}[linenos]{haskell}
op_lift px1 pa1 py1 pa2 py2 adds = do
    op_bar_1 <- mkScanAdjointLam ops (scanLambda scan) WrtFirst (Var . paramName <$> py2)
    let op_bar_args = toExp . Var . paramName <$> px1 ++ pa1
    z_term <- map resSubExp <$> eLambda op_bar_1 op_bar_args
    let z =
        mapM
          (\(z_t, y_1, add) -> head <$> eLambda add [toExp z_t, toExp y_1])
          (zip3 z_term (Var . paramName <$> py1) adds)
    
    let x1 = subExpsRes <$> mapM (toSubExp "x1" . Var . paramName) px1
    op <- renameLambda $ scanLambda scan
    let a3 = eLambda op (toExp . paramName <$> pa1 ++ pa2)
    let is_e = pure [subExpRes $ Constant $ BoolValue False]
    
    concat <$> sequence [x1, a3, z, is_e]
\end{minted}
	\caption{Haskell code for \lstinline{op_lift} (operator for scan calculating adjoints for scan intermediates rs/xs)\\}
	\label{code:hsk-oplift}
\end{code}
ensuring that this operator produces correct results under a scan required more work. This is due to that the constant folding mechanism in later passes of the Futhark compiler made incorrect inferences regarding how the neutral element could be inlined in the scan for some operators and neutral elements. As a concrete example, when differentiating \lstinline{scan f32.max f32.lowest}, the IR generated for \lstinline{op_bar_1} would look like Code Block \ref{code:ir-wrong}

\begin{code}
	\begin{minted}[linenos]{rust}
 \ {...,
    a1 : f32,
    y1_h : f32
    y2_h: f32
    ...} : {..., f32} ->
        let {convop_x_7531 : bool} =
          le32(a1, -f32.inf) // This is suspicious
        let {convop_x_7532 : i32} =
          btoi bool convop_x_7531 to i32
        let {binop_y_7533 : f32} =
          sitofp i32 convop_x_7532 to f32
        let {z_term : f32} =
          fmul32(y2_h, binop_y_7533)
        ...
        let {z : f32} =
          fadd32(y1_h, z_term)
        in {..., z}
\end{minted}
	\caption{Incorrect IR for \lstinline{op_bar_1}}
	\label{code:ir-wrong}
\end{code}
We can see that the original intention of Line 7 in Code Block \ref{code:ir-wrong} would have been to check if $a_1 \leq x_1$
\begin{code}
	\begin{minted}[linenos]{futhark}
let ys = scan op e u
let as_lft = map (\i -> if i < n-1 then u[i+1] else e) (iota n)
let m = zip3 ys as_lft x_b
let (_, _, rs_adj) = unzip3 <|
    scan_right (op_lft plus op) (e, e, zero) m
let ys_right = map (\i -> if i == 0 then e else ys[i-1]) (iota n)
let as_bar = map (op_bar_2 op) (zip3 ys_right u rs_adj)
in  as_bar
\end{minted}
	\caption{Reference Futhark code for \lstinline{scan_bar} (main reverse AD fuction)}
	\label{code:fut-scanbar}
\end{code}
\begin{code}
	\begin{minted}[linenos]{haskell}
let e = scanNeutral scan
as_lift <- asLiftPPAD as w e

is_e <- letExp "is_e" $ BasicOp $ Replicate (Shape [w]) $ Constant $ BoolValue False
let m = ys ++ as_lift ++ ys_adj ++ [is_e]

op_lft <- mkPPADOpLifted ops as scan w
a_zero <- mapM (letExp "rscan_zero" . zeroExp . rowType) as_ts
let lft_scan = Scan op_lft $ e ++ e ++ map Var a_zero ++ [Constant $ BoolValue True]
rs_adj <- (!! 2) . chunk d <$> scanRight m w lft_scan

ys_right <- ysRightPPAD ys w e

final_lmb <- finalMapPPAD ops as scan
letTupExp "as_bar" $ Op $ Screma w (ys_right ++ as ++ rs_adj) $ mapSOAC final_lmb
\end{minted}
	\caption{Haskell code for \lstinline{scan_bar} (main reverse AD fuction)}
	\label{code:hsk-scanbar}
\end{code}
\section{Benchmark}
To compare the efficiency of our implementation,
we have run different benchmarks,
to see how our implementation performs for different operations.
%TODO:Add something about which system the benchmarks are run on.

In our benchmarks, we test 4 different implementations.
\begin{enumerate}

	\item Our own implementation.
	\item An implementation of the same algorithm in Futhark written by Cosmin
	      also for the \cite{Futhark} paper.
	      This is to compare how efficient our implementation is.
	\item The current procedure in Futhark, so we can compare the two
	      different procedures.
	\item The primal code, which performs the scan operation without any AD,
	      to see how big the AD overhead is.

\end{enumerate}

For the purpose of being able to compare with the Benchmarks of
\cite{Futhark}, we have done the same Benchmarks. We have also done further
benchmarks
\begin{figure}
	\begin{tabular}{|c|c|c|c|c|}
		         & Primal & RMAD   & PPAD-compiler & PPAD-futhark \\
		\hline
		10000000 & 7616   & 33828  & 170336        & 45008        \\
		50000000 & 37626  & 167955 & 848794        & 223939
	\end{tabular}
	\caption{Matrix Multiplication 5x5}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD  & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 960    & 6566  & 8263          & 6198         \\
		100000000 & 8986   & 64790 & 81734         & 61005
	\end{tabular}
	\caption{Matrix Multiplication 3x3}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD  & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 370    & 1552  & 2086          & 1579         \\
		100000000 & 3360   & 15019 & 20264         & 15101
	\end{tabular}
	\caption{Matrix Multiplication 2x2}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 188    & 614  & 750           & 746          \\
		100000000 & 1552   & 5637 & 6984          & 6698
	\end{tabular}
	\caption{Linear Function Composition}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 190    & 773  & 641           & 631          \\
		100000000 & 1560   & 7228 & 5919          & 5647
	\end{tabular}
	\caption{Linear Function Composition 2}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD  & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 369    & 2370  & 2744          & 1531         \\
		100000000 & 3248   & 23046 & 26746         & 14510
	\end{tabular}
	\caption{Function Composition}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 144    & 147  & 215           & 157          \\
		100000000 & 784    & 1270 & 1848          & 1325
	\end{tabular}
	\caption{Addition}
	\begin{tabular}{|c|c|c|c|c|}
		         & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		1000000  & 1449   & 302  & 10265         & 1263         \\
		10000000 & 12995  & 2608 & 91315         & 11537
	\end{tabular}
	\caption{Vector Addition}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 146    & 147  & 215           & 565          \\
		100000000 & 1183   & 1270 & 1848          & 4866
	\end{tabular}
	\caption{Min}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 106    & 368  & 479           & 476          \\
		100000000 & 783    & 3229 & 4323          & 4012
	\end{tabular}
	\caption{Mul}
	\begin{tabular}{|c|c|c|c|c|}
		         & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		1000000  & 1448   & 716  & 23582         & 24147        \\
		10000000 & 12968  & 6558 & 214282        & 214121
	\end{tabular}
	\caption{vecmul}
\end{figure}

\subsection{Comparison of PPAD implementation in Futhark and in compiler}
Comparing the two PPAD implementations,
we see that the compiler implementation runs slower or equal in 10/11 benchmarks,
but that for 9 out of 11 benchmarks the runtime is at most double the
runtime of the Futhark implementation.

The two exceptions to these are the matrix Multiplication for 5x5 matrices,
and the vectorised addition,
suggested the code can still be optimised for datatypes consisting of
arrays.
This is however contrasted by the vectorised multiplication which performs
equally well in both implementations.
\subsection{Comparison between PPAD and RMAD procedure}
We see that PPAD-compiler implementation performs worse than the RMAD
implementation, in all cases except for the linear operator on \(\mathbb{R}^{2}\)

\begin{align*}
	(a,b) \odot (c,d) = (a + c + b \cdot d, b + d).
\end{align*}
In a lot of cases this is expected as the vectorised operations, have extra
optimization in \cite{Futhark}, making them run faster.
We still however see that slowdown is no more than a factor 2, for all
benchmarks except 5x5 matrix multiplication, but that seems to an issue with
the compiler implementation, rather than the procedure itself, as the
futhark implementation is much close to the RMAD procedure.
\section{Conclusion}

\printbibliography

\end{document}
