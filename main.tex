\documentclass{article}
% --- Packages ------------------------------------------- ----- Standard Packages --------------------------------
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm} %let us define theorems
\newtheorem{theorem}{Theorem}
\usepackage{amssymb} %Useful Symbols
\usepackage{mathtools} %better version of amsmath
\usepackage{microtype} %make things look nicer
\usepackage{float} %Let figures appear the right place on page
\usepackage{scalerel}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
% https://tex.stackexchange.com/questions/254044/caption-and-label-on-minted-code
\usepackage[newfloat]{minted}
\usepackage{caption}
\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Code Block}
% --- Style for haskell code
\usepackage{listings}
\lstset{
  frame=none,
  xleftmargin=2pt,
  stepnumber=1,
  numbers=left,
  numbersep=5pt,
  numberstyle=\ttfamily\tiny\color[gray]{0.3},
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  escapeinside={*'}{'*},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it,
  stringstyle=\mdseries\rmfamily,
  showspaces=false,
  keywordstyle=\bfseries\rmfamily,
  columns=flexible,
  basicstyle=\small\sffamily,
  showstringspaces=false,
  morecomment=[l]\%,
}
% ---- bibliography ----
\usepackage[backend = biber, style = alphabetic, urldate = long]{biblatex}
\bibliography{References.bib}
% ----- custom commands ----
%\newcommand{\ors}{\overline{\text{rs}}}
%\newcommand{\rs}{\text{rs}}
%\newcommand{\oas}{\overline{\text{as}}}
%\newcommand{\xo}{\overline{\text{x}}}
%\newcommand{\as}{\text{as}}
\newcommand{\ors}{\overline{rs}}
\newcommand{\obs}{\overline{bs}}
\newcommand{\ob}{\overline{b}}
\newcommand{\rs}{rs}
\newcommand{\xs}{rs}
\newcommand{\oas}{\overline{as}}
\newcommand{\xo}{\overline{xs}}
\newcommand{\as}{as}
\newcommand{\bs}{bs}
\newcommand{\cs}{cs}
\newcommand{\ds}{ds}
\title{Scan Reverse-AD Improvements applied to the Futhark Compiler}
\author{Peter Adema (qlz919@alumni.ku.dk)\\
Sophus Valentin Willumsgaard (hwx333@alumni.ku.dk)}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\section{Introduction}
The purpose of this project is to implement the algorithm given in \cite{PPAD}, for reverse
differentiation of the scan operator in Futhark.
In this report,
we describe the procedure for reverse differentiation of a
scan operation given in \cite{PPAD},
and how it differs from the current implementation as described in
\cite{Futhark}.

We then show how we have implemented the procedure in the Futhark compiler,
and lastly give benchmarks to compare with the current implementation.
\section{Description of Algorithm}
We here give a description of the general-case procedures for automatic differentiation of
scan described in \cite{PPAD} and \cite{Futhark}.

We will use \(\odot\) to denote a general associative operator with neutral
element \(e_{\odot}\), defined either on \(\mathbb{R}\) or \(\mathbb{R}^{n}\).
Given an array of variables in the program \(\text{x}\),
we let \(\xo\) denote the array of adjoint values in AD. Following
\cite{PPAD}, and the current implementation of AD in Futhark,
we let the adjoint values have same type as there corresponding variables.
\subsection{Non-Parallel Procedure}
Our task is assuming we have a program containing the following definition
\begin{minted}[escapeinside=||]{futhark}
let |$\rs$| = scan |$\odot$| |$e_{\odot}$| |$\as$|
\end{minted}
to use the values of
\(\ors, \rs, \as\) to calculate \(\oas\).
From the definition of scan, we have
%\begin{minted}[escapeinside=||]{futhark}
%	|$\rs [0] = \as[0]$|
%	|$\rs [i+1] = \rs[i] \odot \as[i+1]$|
%\end{minted} 
\begin{align*}
	\rs[0] = \as[0] \\
	\rs [i+1] = \rs[i] \odot \as[i+1]
\end{align*}
We see that the value of \(\rs[i]\) depend on \(\rs[j]\) for \(i>j\).
For this reason we introduce another array of adjoint vectors \(\xo\),
which is the same as \(\ors\) but with the contribution of the lower
values to the higher values recorded.
From the above formulas we then get

\begin{align*}
	\xo [n-1] = \ors [n-1]     \\
	\xo [i-1] = \frac{\partial (\rs [i-1] \odot \as[i])}{\partial \rs[i-1]}
	\cdot \xo[i]   + \ors[i-1] \\
	\oas [0] = \xo[0]          \\
	\oas [i] = \frac{\partial (\rs [i-1] \odot \as[i])}{\partial \as[i]}
	\cdot
	\xo[i]
\end{align*}
From these formulas, we can calculate \(\oas\) in two loops.
The first loop inductively calculates \(\xo\),
and the second loop calculate \(\oas\),
%TODO:maybe write the for loops in pseudocode?
however this would not preserve parallelism. We will now describe how to
calculate these in a parallel way.
\subsection{Parallelization}
Given the calculation of \(\xo\),
we note that the calculation of \(\oas[i]\) does not depend on any values of
\(\oas\),
so it can be calculated by applying a map operator,
preserving parallelization.

The harder challenge is then to calculate \(\xo\).
To resolve this,
we define a new monoid structure \(\otimes\) on \((\mathbb{R}^{n})^{3}\)
given by

\begin{align*}
	[r_{1}, a_{1}, \overline{r}_{1}] \otimes
	[r_{2}, a_{2}, \overline{r}_{2}] =
	\left[r_{1}, a_{1} \otimes a_{2},
		\frac{\partial (r_{1} \odot a_{1})}{\partial r_{1}} \cdot \overline{r}_{2} + \overline{r}_{1} \right]
\end{align*}
in \cite{PPAD} they show this operator is associative and have a right
identity \(e_{\otimes} :=[e_{\odot}, e_{\odot}, 0]\).

\begin{theorem}
	In the following procedure
	\begin{minted}[escapeinside=££]{futhark}
let £$ms$£ = zip3 £$\rs$£ (tail £$\as$£ ++ £$e_{\odot}$£) £$\ors$£
let (£$\bs,\cs,\ds$£) = scan_right £$\otimes$£ £$e_{\otimes}$£ £$ms$£ |> unzip3
\end{minted}
	\(\ds\) calculates \(\xo\).
\end{theorem}
\begin{proof}
	The argument follows by reverse induction on indices of \(\xo\).
	Starting with \(n-1\), we have
	\begin{align*}
		\ds_{n-1} = \ors_{n-1}
	\end{align*}
	which agrees with  \(\xo_{n-1}\).
	Now assuming that \(\ds_{i} = \xo_{i}\) we have
	\begin{align*}
		[\bs_{i-1}, \cs_{i-1}, \ds_{i-1}] \\
		= [\rs_{i-1}, \as_{i}, \ors_{i-1}]\otimes
		[\bs_{i}, \cs_{i}, \ds_{i}]       \\
		= \left[\rs_{i-1}, a_{i} \otimes \cs_{i},
			\frac{\partial (rs_{i-1} \odot a_{i})}{\partial r_{i-1}} \cdot \ds_{i} + \ors_{i-1}
		\right]                           \\
		= \left[\rs_{i-1}, a_{i} \otimes \cs_{i},
			\frac{\partial (rs_{i-1} \odot a_{i})}{\partial r_{i-1}} \cdot \xo_{i} + \ors_{i-1}
			\right]
	\end{align*}
	We then see that the third argument agrees with our formula for
	\(\xo_{i-1}\) above, finishing the induction argument.
\end{proof}
\(\oas\) is then according to the procedure in\cite{PPAD},
calculated by

\begin{minted}[escapeinside=££]{futhark}
let £$ms$£ = zip3 £$\rs$£ (tail £$\as$£ ++ [£$e_{\odot}$£]) £$\ors$£
let (£$\bs,\cs,\ds$£) = scan_right £$\otimes$£ £$e_{\otimes}$£ £$ms$£ |> unzip3
map3 (\x y z -> £$\partial (x \odot y)/\partial y \cdot z$£) ([£$e_{\odot}$£] ++ init £$\rs$£ ) £$ \as $£ £$ \ds $£
\end{minted}
\subsection{Comparison to current Futhark implementation}
The general idea for reverse AD for scan operators in the current Futhark
implementation, is similar to the above procedure,
in that it first calculates the array \(\xo\) with a scan,
and then calculates \(\oas\) with a map using \(\xo\).

The calculation of \(\oas\) is the same as the one above,
but differs in the calculation of \(\xo\).

Instead of calculating the derivatives, when they are applied to the
adjoints inside the scan,
it instead first calculates all the derivatives

\begin{align*}
	\frac{\partial rs[i-1] \odot \as[i]}{\partial \as[i]}
\end{align*}
and saves the result. This can be done by a map operator, since the
derivative only depend on \(\rs, \as\) and the operator \(\odot\).
From this \(\xo\) can be calculated by a scan operator.

It is also worth noting that in special cases,
such as then the operator \(\odot\) is addition or a vectorised version of scalar operator,
there are more optimal ways of calculating the adjoint values,
which the compiler takes advantage of.
\subsection{Discussion about Efficiency}
The disadvantages of calculating the Jacobians beforehand,
is that it is memory-intensive to have so much data to be used inside the
scan operator. Furthermore calculating the Jacobians seperated from their
application to the adjoints,
removes the possibility of using optimization built in the automatic
differentiation.
For example if the adjoint values are 0 in some dimensions,
the derivatives does not need to be calculated in those dimensions.

However calculating the Jacobian beforehand has some advantages. In
particular,
the current Futhark compiler, after calculating the Jacobian, performs static
analysis to see if the Jacobian is of block-diagonal form.
In this case the Jacobians can be represented by much smaller matrices,
which vastly increases the scan step in computation.
\newpage
\section{Implementation}
The implementation of the algorithm described in \cite{PPAD} was driven by the \href{https://github.com/diku-dk/ifl23-revad-red-scan/blob/main/scan/scan-adj-comp.fut}{reference implementation} in Futhark code from \cite{Futhark}, with the structure of the compiler code globally following the Futhark reference code. There is one notable difference between the Futhark implementation and the compiler code, namely in the treatment of neutral elements for \lstinline{op_lifted}. In \cite{PPAD} it was noted that for the lifted operator as defined there, \lstinline{(e, e, 0)} would serve as a right-neutral element, allowing for a reverse-scan over the constructed array \lstinline{m} (with \lstinline{e} being the original neutral element). This approach leads to the simple definition of \lstinline{op_lift} from the Futhark reference code in Code Block \ref{code:fut-oplift}:
\begin{code}
	\begin{minted}[linenos]{futhark}
let z_term = op_bar_1 op (x1, a1, y2_h)
let z = plus z_term y1_h
in  (x1, op a1 a2, z)
\end{minted}
	\caption{Reference Futhark code for \lstinline{op_lift} (operator for scan calculating adjoints for scan intermediates rs/xs). \lstinline{op_bar_1} refers to differentiating \lstinline{op} with respect to its first argument.\\}
	\label{code:fut-oplift}
\end{code}
We can similarly define the core operator in the compiler code, as shown in Code Block \ref{code:hsk-oplift}:
\begin{code}
	\begin{minted}[linenos]{haskell}
op_lift px1 pa1 py1 pa2 py2 adds = do
    op_bar_1 <- mkScanAdjointLam ops (scanLambda scan) 
                    WrtFirst (Var . paramName <$> py2)
    let op_bar_args = toExp . Var . paramName <$> px1 ++ pa1
    z_term <- map resSubExp <$> eLambda op_bar_1 op_bar_args
    let z =
        mapM
          (\(z_t, y_1, add) -> head <$> eLambda add [toExp z_t, toExp y_1])
          (zip3 z_term (Var . paramName <$> py1) adds)
    
    let x1 = subExpsRes <$> mapM (toSubExp "x1" . Var . paramName) px1
    op <- renameLambda $ scanLambda scan
    let a3 = eLambda op (toExp . paramName <$> pa1 ++ pa2)
    
    concat <$> sequence [x1, a3, z]
\end{minted}
	\caption{Haskell code for \lstinline{op_lift} (operator for scan calculating adjoints for scan intermediates rs/xs).\\}
	\label{code:hsk-oplift}
\end{code}
However, while the above is an accurate translation of the Futhark reference code, ensuring that it produces correct results under a scan required more work. This is due to that the constant folding mechanism in later passes of the Futhark compiler occasionally made incorrect inferences regarding how the neutral element could be inlined in the scan. As a concrete example, when differentiating \lstinline{scan f32.max f32.lowest}, the IR generated for \lstinline{op_bar_1} would look like Code Block \ref{code:ir-wrong}:

\begin{code}
	\begin{minted}[linenos]{rust}
// This lambda is op_lift for f32.max, as an argument to scanomap
 \ {...,
    a1 : f32,
    y1_h : f32,
    y2_h: f32
    ...} : {..., f32} ->
        let {convop_x_7531 : bool} =
          le32(a1, -f32.inf) // x1 was replaced with the n.e.
        let {convop_x_7532 : i32} =
          btoi bool convop_x_7531 to i32
        let {der_x1 : f32} =
          sitofp i32 convop_x_7532 to f32
        let {z_term : f32} =
          fmul32(y2_h, der_x1)
        ...
        let {z : f32} =
          fadd32(y1_h, z_term)
        in {..., z}
\end{minted}
	\caption{Incorrect IR for \lstinline{op_bar_1}. Names have been changed for clarity.\\}
	\label{code:ir-wrong}
\end{code}
\vspace{0.3cm}
We can see that the original intention of Line 8 in Code Block \ref{code:ir-wrong} would have been to check if $a_1 \leq x_1$, as in that case the carry $x_1$ would have continued through the \lstinline{f32.max}, adding the contribution of \lstinline{y2_h} to \lstinline{z} in the form of \lstinline{z_term}. However, we can see that, during optimization of the generated function, $x_1$ was replaced with the constant value from the neutral value (\lstinline{f32.lowest} is \lstinline{-f32.inf}).

To mitigate this, the method we found was to add an extra boolean field to all elements of the scan, which would be \lstinline{True} for the neutral element and \lstinline{False} for all elements from the real array. Modifying \lstinline{op_lift} such that it ignored neutral elements and only performed the calculations when both of its arguments were non-neutral elements added overhead to the operator, but prevented the incorrect inlining from taking place. Notably, adding this check to only one side (i.e. only guarding against neutral elements as a first argument) fixed some instances of incorrect inlining (such as \lstinline{scan f32.max}, and even though the reverse scan makes it such that neutral elements should only appear as second argument), but generated new failures as well (causing the scan of matrix multiplication to fail). As such, a guard against neutral elements was necessary on both sides.

Using this implementation of \lstinline{op_lift} (named \lstinline{mkPPADOpLifted} in the compiler code), we proceeded to implement the main body of the scan reverse-AD. Code Block \ref{code:fut-scanbar} shows the reference code for this (named \lstinline{scan_bar}):
\newpage
\begin{code}
	\begin{minted}[linenos]{futhark}
let ys = scan op e u
let as_lft = map (\i -> if i < n-1 then u[i+1] else e) (iota n)
let m = zip3 ys as_lft x_b
let (_, _, rs_adj) = unzip3 <|
    scan_right (op_lft plus op) (e, e, zero) m
let ys_right = map (\i -> if i == 0 then e else ys[i-1]) (iota n)
let as_bar = map (op_bar_2 op) (zip3 ys_right u rs_adj)
in  as_bar
\end{minted}
	\caption{Reference Futhark code for \lstinline{scan_bar} (main reverse AD fuction).}
	\label{code:fut-scanbar}
\end{code}
\vspace{0.3cm}
The compiler code is very similar in structure, with the exception of the additional boolean field marking the neutral element as "e" and the original array as "not e". The following Code Block \ref{code:hsk-scanbar} was added as part of the main \lstinline{diffScan} function (each line grouping is one \lstinline{let} from Code Block \ref{code:fut-scanbar}):
\begin{code}
	\begin{minted}[linenos]{haskell}
let e = scanNeutral scan
as_lift <- asLiftPPAD as w e

isnt_e <- letExp "isnt_e" $ BasicOp $ Replicate (Shape [w]) $ Constant $ BoolValue False
let m = ys ++ as_lift ++ ys_adj ++ [isnt_e]

op_lft <- mkPPADOpLifted ops as scan w
a_zero <- mapM (fmap Var . letExp "rscan_zero" . zeroExp . rowType) as_types
let lft_scan = Scan op_lft $ e ++ e ++ a_zero ++ [Constant $ BoolValue True]
rs_adj <- (!! 2) . chunk d <$> scanRight m w lft_scan

ys_right <- ysRightPPAD ys w e

op_bar_2 <- finalMapPPAD ops as scan
letTupExp "as_bar" $ Op $ Screma w (ys_right ++ as ++ rs_adj) $ mapSOAC op_bar_2
\end{minted}
	\caption{Haskell code for \lstinline{scan_bar} (main reverse AD fuction). \lstinline{w} is the length of the arrays in \lstinline{as}.\\}
	\label{code:hsk-scanbar}
\end{code}
\vspace{0.3cm}
The functions \lstinline{asLiftPPAD}, \lstinline{scanRight}, \lstinline{ysRightPPAD} and \lstinline{finalMapPPAD} were also added in the compiler implementation, all bearing close resemblance to the reference implementation in terms of structure.

Finally, we would like to remark that in some cases, the utility function \lstinline{lambdaReturnType} gave element types instead of the true returns types; returning \lstinline{f32} when a lambda actually returned \lstinline{[]f32}. While there was a relatively simple workaround for this bug, it should be noted that these incorrect types currently cause compilation errors in the 'normal' scan reverse-AD compiler (see \href{https://gist.github.com/p-adema/c8fc4fd5823baca18d90385370aeda5d}{this gist}).

\newpage
\section{Benchmark}
To compare the efficiency of our implementation,
we have run different benchmarks,
to see how our implementation performs for different operations.
%TODO:Add something about which system the benchmarks are run on.

In our benchmarks, we test 4 different implementations.
\begin{enumerate}

	\item Our own implementation.
	\item An implementation of the same PPAD algorithm in Futhark written by Cosmin
	      also for the \cite{Futhark} paper.
	      This is to compare how efficient our implementation is.
	\item The current procedure in Futhark, so we can compare the two
	      different procedures.
	\item The primal code, which performs the scan operation without any AD,
	      to measure the AD overhead.

\end{enumerate}

For the purpose of being able to compare with the Benchmarks of
\cite{Futhark}, we have done the same Benchmarks. We have also done further
benchmarks
\begin{figure}
	\begin{tabular}{|c|c|c|c|c|}
		         & Primal & RMAD   & PPAD-compiler & PPAD-futhark \\
		\hline
		10000000 & 7616   & 33828  & 170336        & 45008        \\
		50000000 & 37626  & 167955 & 848794        & 223939
	\end{tabular}
	\caption{Matrix Multiplication 5x5}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD  & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 960    & 6566  & 8263          & 6198         \\
		100000000 & 8986   & 64790 & 81734         & 61005
	\end{tabular}
	\caption{Matrix Multiplication 3x3}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD  & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 370    & 1552  & 2086          & 1579         \\
		100000000 & 3360   & 15019 & 20264         & 15101
	\end{tabular}
	\caption{Matrix Multiplication 2x2}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 188    & 614  & 750           & 746          \\
		100000000 & 1552   & 5637 & 6984          & 6698
	\end{tabular}
	\caption{Linear Function Composition}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 190    & 773  & 641           & 631          \\
		100000000 & 1560   & 7228 & 5919          & 5647
	\end{tabular}
	\caption{Linear Function Composition 2}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD  & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 369    & 2370  & 2744          & 1531         \\
		100000000 & 3248   & 23046 & 26746         & 14510
	\end{tabular}
	\caption{Function Composition}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 144    & 147  & 215           & 157          \\
		100000000 & 784    & 1270 & 1848          & 1325
	\end{tabular}
	\caption{Addition}
	\begin{tabular}{|c|c|c|c|c|}
		         & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		1000000  & 1449   & 302  & 10265         & 1263         \\
		10000000 & 12995  & 2608 & 91315         & 11537
	\end{tabular}
	\caption{Vector Addition}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 146    & 147  & 215           & 565          \\
		100000000 & 1183   & 1270 & 1848          & 4866
	\end{tabular}
	\caption{Min}
	\begin{tabular}{|c|c|c|c|c|}
		          & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		10000000  & 106    & 368  & 479           & 476          \\
		100000000 & 783    & 3229 & 4323          & 4012
	\end{tabular}
	\caption{Mul}
	\begin{tabular}{|c|c|c|c|c|}
		         & Primal & RMAD & PPAD-compiler & PPAD-futhark \\
		\hline

		1000000  & 1448   & 716  & 23582         & 24147        \\
		10000000 & 12968  & 6558 & 214282        & 214121
	\end{tabular}
	\caption{vecmul}
\end{figure}

\subsection{Comparison of PPAD implementation in Futhark and in compiler}
Comparing the two PPAD implementations,
we see that the compiler implementation runs slower or equal in 10/11 benchmarks,
but that for 9 out of 11 benchmarks the runtime is at most double the
runtime of the Futhark implementation.

The two exceptions to these are the matrix Multiplication for 5x5 matrices,
and the vectorised addition,
suggested the code can still be optimised for datatypes consisting of
arrays.
This is however contrasted by the vectorised multiplication which performs
equally well in both implementations.

\subsection{Comparison between PPAD and RMAD procedure}
We see that PPAD-compiler implementation performs worse than the RMAD
implementation, in all cases except for the linear operator on \(\mathbb{R}^{2}\)

\begin{align*}
	(a,b) \odot (c,d) = (a + c + b \cdot d, b + d).
\end{align*}
In a lot of cases this is expected as the vectorised operations, have extra
optimization in \cite{Futhark}, making them run faster.
We still however see that slowdown is no more than a factor 2, for all
benchmarks except 5x5 matrix multiplication, but that seems to an issue with
the compiler implementation, rather than the procedure itself, as the
futhark implementation is much close to the RMAD procedure.
\section{Conclusion}
We can see that ...

We suspect that the cause for the lacking performance in our compiler-generated version of the PPAD
algorithm lies in the additional checks for neutral elements that are inserted around
every call to \lstinline{op_lifted}, but verification of this would require a correct
compiler implementation without these checks.

In conclusion, using the PPAD algorithm as an alternative for the current implementation in the
Futhark compiler holds promise, but due to practical limitations and lacking performance, further
work would need to be done to make it a viable alternative in terms of performance.
\printbibliography

\end{document}
